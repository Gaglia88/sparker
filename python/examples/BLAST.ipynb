{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1533d2",
   "metadata": {},
   "source": [
    "# BLAST\n",
    "\n",
    "This example illustrates how to perform meta-blocking by using BLAST.\n",
    "\n",
    "BLAST can be only used on clean-clean datasets (data linkage), so when we have two datasets that do not contain duplicates and we want to discover the duplicates between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3219bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparker\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b84809",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "sparkER provides wrappers to load CSV and JSON files.\n",
    "\n",
    "First, we load the first dataset, and we extract the maximum id. The profiles ids of the second dataset will be assigned starting from this one.\n",
    "\n",
    "*real_id_field* is the field that contains the identifier of the record.\n",
    "*source_id* is used to identify from which dataset the profile belongs, it is necessary for the attributes alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e6600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiles contained in the first dataset\n",
    "profiles1 = sparker.JSONWrapper.load_profiles('../datasets/clean/DblpAcm/dataset1.json', \n",
    "                                              real_id_field = \"realProfileID\", \n",
    "                                              source_id=1)\n",
    "# Max profile id in the first dataset, used to separate the profiles in the next phases\n",
    "separator_id = profiles1.map(lambda profile: profile.profile_id).max()\n",
    "# Separators, used during blocking to understand from which dataset a profile belongs. It is an array because sparkER\n",
    "# could work with multiple datasets\n",
    "separator_ids = [separator_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b54c1",
   "metadata": {},
   "source": [
    "Let's visualize a profile to check if they are correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ab72f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'profile_id': 0, 'attributes': [{'key': 'venue', 'value': 'SIGMOD Record'}, {'key': 'year', 'value': '1999'}, {'key': 'title', 'value': 'Semantic Integration of Environmental Models for Application to Global Information Systems and Decision-Making'}, {'key': 'authors', 'value': 'D. Scott Mackay'}], 'original_id': '0', 'source_id': 1}\n"
     ]
    }
   ],
   "source": [
    "print(profiles1.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba53b12",
   "metadata": {},
   "source": [
    "Loads the second dataset and extract the max id (it will be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef815292",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles2 = sparker.JSONWrapper.load_profiles('../datasets/clean/DblpAcm/dataset2.json', \n",
    "                                              start_id_from = separator_id+1, \n",
    "                                              real_id_field = \"realProfileID\", \n",
    "                                              source_id=2)\n",
    "# Max profile id\n",
    "max_profile_id = profiles2.map(lambda profile: profile.profile_id).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf0640",
   "metadata": {},
   "source": [
    "Finally, concatenate the two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d9f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = profiles1.union(profiles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf8051",
   "metadata": {},
   "source": [
    "### Groundtruth (optional)\n",
    "If you have a groundtruth you can measure the performance at each blocking step.\n",
    "\n",
    "When you load the groundtruth, it contains the original profiles IDs, so it is necessary to convert it to use the IDs assigned to each profile by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fe0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the groundtruth, takes as input the path of the file and the names of the attributes that represent\n",
    "# respectively the id of profiles of the first dataset and the id of profiles of the second dataset\n",
    "gt = sparker.JSONWrapper.load_groundtruth('../datasets/clean/DblpAcm/groundtruth.json', 'id1', 'id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5183131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the groundtruth by replacing original IDs with those given by Spark\n",
    "new_gt = sparker.Converters.convert_groundtruth(gt, profiles1, profiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef8f4fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(51, 4816), (1496, 4445)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can explore some pairs\n",
    "random.sample(new_gt, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33453fac",
   "metadata": {},
   "source": [
    "## Attributes alignment\n",
    "\n",
    "BLAST employs LSH to automatically align the attributes.\n",
    "\n",
    "If *compute_entropy* is set to True it computes the entropy of each cluster of attributes that can be used to further improve the meta-blocking performance.\n",
    "\n",
    "*target_threshold* parameter regulates the similarity that the values of the attributes should have to be clustered together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df86ea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_id': 0, 'keys': ['2_title', '1_title'], 'entropy': 9.280896484656086},\n",
       " {'cluster_id': 1, 'keys': ['1_authors', '2_authors'], 'entropy': 10.71793208243355},\n",
       " {'cluster_id': 2, 'keys': ['1_year', '2_year'], 'entropy': 3.309290823680249},\n",
       " {'cluster_id': 3, 'keys': ['2_venue', '1_venue'], 'entropy': 3.3225607247542883}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters = sparker.AttributeClustering.cluster_similar_attributes(profiles,\n",
    "                                  num_hashes=128,\n",
    "                                  target_threshold=0.5,\n",
    "                                  compute_entropy=True)\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232a623",
   "metadata": {},
   "source": [
    "## Blocking\n",
    "Now we can perform blocking by using the generated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074a678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks 7120\n"
     ]
    }
   ],
   "source": [
    "blocks = sparker.Blocking.create_blocks_clusters(profiles, clusters, separator_ids)\n",
    "print(\"Number of blocks\",blocks.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca826a0b",
   "metadata": {},
   "source": [
    "## Block cleaning\n",
    "\n",
    "sparkER implements two block cleaning strategies:\n",
    "\n",
    "* Block purging: discard the largest blocks that involve too many comparisons, the parameter must be >= 1. A lower value mean a more aggressive purging.\n",
    "* Block cleaning: removes for every profile the largest blocks in which it appears. The parameter is in range ]0, 1\\[. A lower value mean a more aggressive cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0be26d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfoms the purging\n",
    "blocks_purged = sparker.BlockPurging.block_purging(blocks, 1.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba2d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs the cleaning\n",
    "(profile_blocks, profile_blocks_filtered, blocks_after_filtering) = sparker.BlockFiltering.block_filtering_quick(blocks_purged, 0.8, separator_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981401c0",
   "metadata": {},
   "source": [
    "If you have the groundtruth, after every blocking step it is possible to check which are the performance of the blocking collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe5c7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 1.0\n",
      "Precision 0.03026509172064667\n",
      "Number of comparisons 73484\n"
     ]
    }
   ],
   "source": [
    "recall, precision, cmp_n = sparker.Utils.get_statistics(blocks_after_filtering, max_profile_id, new_gt, separator_ids)\n",
    "\n",
    "print(\"Recall\", recall)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Number of comparisons\", cmp_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a531a8",
   "metadata": {},
   "source": [
    "## Meta-blocking\n",
    "Meta-blocking can be used to further refine the block collection removing superfluous comparisons.\n",
    "\n",
    "\n",
    "For every partition of the RDD, the pruning algorithm returns as output a triplet that contains:\n",
    "\n",
    "* The number of edges\n",
    "* The number of matches (only if the groundtruth is provided)\n",
    "* The retained edges\n",
    "\n",
    "To perform the meta-blocking first some data structures have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b57934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index_map = blocks_after_filtering.map(lambda b : (b.block_id, b.profiles)).collectAsMap()\n",
    "block_index = sc.broadcast(block_index_map)\n",
    "\n",
    "block_entropies = sc.broadcast(blocks.map(lambda b : (b.block_id, b.entropy)).collectAsMap())\n",
    "\n",
    "# This is only needed for certain weight measures\n",
    "profile_blocks_size_index = sc.broadcast(profile_blocks_filtered.map(lambda pb : (pb.profile_id, len(pb.blocks))).collectAsMap())\n",
    "\n",
    "# Broadcasted groundtruth\n",
    "gt_broadcast = sc.broadcast(new_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647641b0",
   "metadata": {},
   "source": [
    "### Meta-blocking with BLAST\n",
    "BLAST employs $\\chi^2$ weighting scheme.\n",
    "\n",
    "*chi2divider* parameter regulates the pruning aggressivity, a lower value performs a more aggressive pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf414b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9986510791366906\n",
      "Precision 0.31924680178237747\n",
      "Number of comparisons 6957\n"
     ]
    }
   ],
   "source": [
    "results = sparker.WNP.wnp(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CHI_SQUARE,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          use_entropy=True,\n",
    "                          blocks_entropies=block_entropies,\n",
    "                          chi2divider=2.0\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a09f1",
   "metadata": {},
   "source": [
    "## Collecting edges after meta-blocking\n",
    "As mentioned before, the third element of the tuples returned by the meta-blocking contains the edges.\n",
    "\n",
    "\n",
    "Edges are weighted according to the weight strategy provided to the meta-blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c96106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2733, 474746.74542385637),\n",
       " (404, 4257, 323134.03437346633),\n",
       " (404, 4292, 377061.62799117394),\n",
       " (1448, 3092, 533286.1049898568),\n",
       " (1448, 2689, 28001.924248551786),\n",
       " (2612, 2818, 9460.396018690539),\n",
       " (2412, 3616, 634811.2979628429),\n",
       " (2412, 3037, 24872.747932579347),\n",
       " (1284, 3052, 65034.464088759465),\n",
       " (1284, 2686, 859823.5641014529)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = results.flatMap(lambda x: x[2])\n",
    "\n",
    "edges.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d86827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
