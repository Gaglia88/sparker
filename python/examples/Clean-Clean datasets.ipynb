{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b1533d2",
   "metadata": {},
   "source": [
    "# Clean - Clean Dataset\n",
    "This example illustrate how to perform meta-blocking on a clean-clean dataset (data linkage), so when we have two datasets that do not contain duplicates and we want to discover the duplicates between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3219bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparker\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b84809",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "sparkER provides wrappers to load CSV and JSON files.\n",
    "\n",
    "First, we load the first dataset, and we extract the maximum id. The profiles ids of the second dataset will be assigned starting from this one.\n",
    "\n",
    "*real_id_field* is the field that contains the identifier of the record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1e6600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiles contained in the first dataset\n",
    "profiles1 = sparker.JSONWrapper.load_profiles('../datasets/clean/abtBuy/dataset1.json', \n",
    "                                              real_id_field = \"realProfileID\")\n",
    "# Max profile id in the first dataset, used to separate the profiles in the next phases\n",
    "separator_id = profiles1.map(lambda profile: profile.profile_id).max()\n",
    "# Separators, used during blocking to understand from which dataset a profile belongs. It is an array because sparkER\n",
    "# could work with multiple datasets\n",
    "separator_ids = [separator_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b54c1",
   "metadata": {},
   "source": [
    "Let's visualize a profile to check if they are correctly loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ab72f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'profile_id': 0, 'attributes': [{'key': 'name', 'value': 'Sony Turntable - PSLX350H'}, {'key': 'description', 'value': 'Sony Turntable - PSLX350H/ Belt Drive System/ 33-1/3 and 45 RPM Speeds/ Servo Speed Control/ Supplied Moving Magnet Phono Cartridge/ Bonded Diamond Stylus/ Static Balance Tonearm/ Pitch Control'}], 'original_id': '0', 'source_id': 0}\n"
     ]
    }
   ],
   "source": [
    "print(profiles1.take(1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba53b12",
   "metadata": {},
   "source": [
    "Loads the second dataset and extract the max id (it will be used later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef815292",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles2 = sparker.JSONWrapper.load_profiles('../datasets/clean/abtBuy/dataset2.json', \n",
    "                                              start_id_from = separator_id+1, \n",
    "                                              real_id_field = \"realProfileID\")\n",
    "# Max profile id\n",
    "max_profile_id = profiles2.map(lambda profile: profile.profile_id).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcf0640",
   "metadata": {},
   "source": [
    "Finally, concatenate the two RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d9f2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = profiles1.union(profiles2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baf8051",
   "metadata": {},
   "source": [
    "### Groundtruth (optional)\n",
    "If you have a groundtruth you can measure the performance of each step.\n",
    "\n",
    "When you load the groundtruth it contains the original profiles IDs, it is necessary to convert it to use the IDs assigned to each profile by Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0fe0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the groundtruth, takes as input the path of the file and the names of the attributes that represent\n",
    "# respectively the id of profiles of the first dataset and the id of profiles of the second dataset\n",
    "gt = sparker.JSONWrapper.load_groundtruth('../datasets/clean/abtBuy/groundtruth.json', 'id1', 'id2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5183131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the groundtruth by replacing original IDs with those given by Spark\n",
    "new_gt = sparker.Converters.convert_groundtruth(gt, profiles1, profiles2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef8f4fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(373, 1254), (23, 1555)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can explore some pairs\n",
    "random.sample(new_gt, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232a623",
   "metadata": {},
   "source": [
    "## Blocking\n",
    "Now we can perform blocking.\n",
    "\n",
    "By default sparkER performs token blocking, but it is possible to provide a different blocking function.\n",
    "\n",
    "In the following example each token is splitted in ngrams of size 4 that are used for blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "074a678d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks 9159\n"
     ]
    }
   ],
   "source": [
    "blocks = sparker.Blocking.create_blocks(profiles, separator_ids, \n",
    "                                        blocking_method=sparker.BlockingKeysStrategies.ngrams_blocking,\n",
    "                                        ngram_size=4)\n",
    "print(\"Number of blocks\",blocks.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b4a70",
   "metadata": {},
   "source": [
    "Let's continue by using the standard token blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "099e56b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of blocks 2132\n"
     ]
    }
   ],
   "source": [
    "blocks = sparker.Blocking.create_blocks(profiles, separator_ids)\n",
    "print(\"Number of blocks\",blocks.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca826a0b",
   "metadata": {},
   "source": [
    "## Block cleaning\n",
    "\n",
    "sparkER implements two block cleaning strategies:\n",
    "\n",
    "* Block purging: discard the largest blocks that involve too many comparisons, the parameter must be >= 1. A lower value mean a more aggressive purging.\n",
    "* Block cleaning: removes for every profile the largest blocks in which it appears. The parameter is in range ]0, 1\\[. A lower value mean a more aggressive cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0be26d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfoms the purging\n",
    "blocks_purged = sparker.BlockPurging.block_purging(blocks, 1.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fba2d81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs the cleaning\n",
    "(profile_blocks, profile_blocks_filtered, blocks_after_filtering) = sparker.BlockFiltering.block_filtering_quick(blocks_purged, 0.8, separator_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981401c0",
   "metadata": {},
   "source": [
    "If you have the groundtruth, after every blocking step it is possible to check which are the performance of the blocking collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe5c7530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9953531598513011\n",
      "Precision 0.009352813266847726\n",
      "Number of comparisons 114511\n"
     ]
    }
   ],
   "source": [
    "recall, precision, cmp_n = sparker.Utils.get_statistics(blocks_after_filtering, max_profile_id, new_gt, separator_ids)\n",
    "\n",
    "print(\"Recall\", recall)\n",
    "print(\"Precision\", precision)\n",
    "print(\"Number of comparisons\", cmp_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a531a8",
   "metadata": {},
   "source": [
    "## Meta-blocking\n",
    "Meta-blocking can be used to further refine the block collection removing superfluous comparisons.\n",
    "\n",
    "SparkER implements different kind of meta-blocking algorithms, you can find the descriptions in our paper.\n",
    "\n",
    "\n",
    "For every partition of the RDD the pruning algorithm returns as output a triplet that contains:\n",
    "\n",
    "* The number of edges\n",
    "* The number of matches (only if the groundtruth is provided)\n",
    "* The retained edges\n",
    "\n",
    "To perform the meta-blocking first some data structures have to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b57934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_index_map = blocks_after_filtering.map(lambda b : (b.block_id, b.profiles)).collectAsMap()\n",
    "block_index = sc.broadcast(block_index_map)\n",
    "\n",
    "# This is only needed for certain weight measures\n",
    "profile_blocks_size_index = sc.broadcast(profile_blocks_filtered.map(lambda pb : (pb.profile_id, len(pb.blocks))).collectAsMap())\n",
    "\n",
    "# Broadcasted groundtruth\n",
    "gt_broadcast = sc.broadcast(new_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647641b0",
   "metadata": {},
   "source": [
    "### Weighted Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcf414b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9693308550185874\n",
      "Precision 0.036182612918892666\n"
     ]
    }
   ],
   "source": [
    "results = sparker.WNP.wnp(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.OR\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec31755",
   "metadata": {},
   "source": [
    "### Reciprocal Weighted Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fd75b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9628252788104089\n",
      "Precision 0.03915047993348953\n"
     ]
    }
   ],
   "source": [
    "results = sparker.WNP.wnp(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.AND\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07c3e6",
   "metadata": {},
   "source": [
    "### Weighted Edge Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32079453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9618959107806692\n",
      "Precision 0.03620906801007557\n"
     ]
    }
   ],
   "source": [
    "results = sparker.WEP.wep(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d580626",
   "metadata": {},
   "source": [
    "### Cardinality Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "399c6c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.9600371747211895\n",
      "Precision 0.05844743691297952\n"
     ]
    }
   ],
   "source": [
    "results = sparker.CNP.cnp(\n",
    "                          blocks_after_filtering,\n",
    "                          profiles.count(),\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.OR\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f89c",
   "metadata": {},
   "source": [
    "### Reciprocal Cardinality Node Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4713f7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.8485130111524164\n",
      "Precision 0.15801315334025615\n",
      "Number of comparisons 5778\n"
     ]
    }
   ],
   "source": [
    "results = sparker.CNP.cnp(\n",
    "                          blocks_after_filtering,\n",
    "                          profiles.count(),\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index,\n",
    "                          comparison_type=sparker.ComparisonTypes.AND\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5285b5",
   "metadata": {},
   "source": [
    "### Cardinality Edge Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c541f9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.8680297397769516\n",
      "Precision 0.06737844466887895\n",
      "Number of comparisons 13862\n"
     ]
    }
   ],
   "source": [
    "results = sparker.CEP.cep(\n",
    "                          profile_blocks_filtered,\n",
    "                          block_index,\n",
    "                          max_profile_id,\n",
    "                          separator_ids,\n",
    "                          weight_type=sparker.WeightTypes.CBS,\n",
    "                          groundtruth=gt_broadcast,\n",
    "                          profile_blocks_size_index=profile_blocks_size_index\n",
    "                         )\n",
    "num_edges = results.map(lambda x: x[0]).sum()\n",
    "num_matches = results.map(lambda x: x[1]).sum()\n",
    "print(\"Recall\", num_matches/len(new_gt))\n",
    "print(\"Precision\", num_matches/num_edges)\n",
    "print(\"Number of comparisons\",num_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a09f1",
   "metadata": {},
   "source": [
    "## Collecting edges after meta-blocking\n",
    "As mentioned before, the third element of the tuples returned by the meta-blocking contains the edges.\n",
    "\n",
    "\n",
    "Edges are weighted according to the weight strategy provided to the meta-blocking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c96106d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1281, 2),\n",
       " (0, 1773, 2),\n",
       " (0, 1269, 2),\n",
       " (0, 1129, 2),\n",
       " (572, 1773, 4),\n",
       " (572, 1385, 3),\n",
       " (572, 1940, 3),\n",
       " (572, 2064, 2),\n",
       " (572, 2070, 2),\n",
       " (572, 2062, 2)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges = results.flatMap(lambda x: x[2])\n",
    "\n",
    "edges.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d86827",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
